{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3859ea1b-d792-43c1-91fe-4488caa9d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    PeftModel,\n",
    "    prepare_model_for_kbit_training,\n",
    "    get_peft_model,\n",
    ")\n",
    "import os, torch, wandb\n",
    "from trl import SFTTrainer, setup_chat_format\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30ea88ef-d59d-4f85-9a4f-33aa2831b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/translate_many_category_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eed73c09-decc-4c00-9125-e660d4e6e612",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"NousResearch/Meta-Llama-3.1-8B-Instruct\"\n",
    "#new_model = \"Llama3.1-ruFinGPT\"\n",
    "\n",
    "#model_name = \"t-bank-ai/T-lite-instruct-0.1\"\n",
    "#new_model = \"T-lite-ruFinGPT\"\n",
    "\n",
    "model_name = \"IlyaGusev/saiga_llama3_8b\"\n",
    "new_model = \"saiga_llama3_8b-ruFinGPT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f863365a-acde-439c-8dd7-3a74cffe392f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 18\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mbnb_config = BitsAndBytesConfig(\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m    load_in_4bit=True,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m---> 18\u001b[0m         base_model,\n\u001b[0;32m     19\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     20\u001b[0m         low_cpu_mem_usage\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     21\u001b[0m         torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[0;32m     22\u001b[0m         device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     23\u001b[0m         trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     24\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'base_model' is not defined"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=\"eager\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baa78d8-ac88-4f25-95f6-c141c43ac5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf042c9-9a6f-4ca4-afa1-39608c4c93d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "def format_chat_template(row):\n",
    "    row = row[1]\n",
    "    row_json = [\n",
    "        {\"role\": \"system\", \"content\": row[\"instruction\"]},\n",
    "        {\"role\": \"user\", \"content\": row[\"input\"]},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"output\"]},         \n",
    "    ]\n",
    "\n",
    "    return tokenizer.apply_chat_template(row_json, tokenize=False)\n",
    "\"\"\"\n",
    "\n",
    "def format_chat_template(row):\n",
    "    row = row[1]\n",
    "    row_json = [\n",
    "        {\"role\": \"system\", \"content\": row[\"instruction\"]},\n",
    "        {\"role\": \"user\", \"content\": row[\"input\"]},\n",
    "        {\"role\": \"assistant\", \"content\": row[\"output\"]},         \n",
    "    ]\n",
    "    res_text = \"<|begin_of_text|>\"\n",
    "    for item in row_json:\n",
    "        res_text += f\"<|start_header_id|>{item['role']}<|end_header_id|>{item['content']}<|eot_id|>\"\n",
    "        \n",
    "    return res_text\n",
    "\n",
    "df['text'] = [format_chat_template(row) for row in df.iterrows()]\n",
    "df['text'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f044bd-373e-4407-af24-593765312c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=new_model,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    learning_rate=2e-4,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    group_by_length=True,\n",
    "    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4f5de-f300-498e-9d24-cb1858ca179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=512,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing= False,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0a1f752d-490e-4ca6-a043-4f5fa16431ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▅▃▁</td></tr><tr><td>eval/runtime</td><td>█▁▁▃</td></tr><tr><td>eval/samples_per_second</td><td>▁██▆</td></tr><tr><td>eval/steps_per_second</td><td>▁██▆</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>▃▃▂▃▄▃▄▃▄▃▃▁█▂▂▂▃▄▄▂▃▃▁▃▅▄▃▂▂▇▃▂▃▄▂▂▃▃▃▅</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>▆▃▃▄▅▅█▃▆▆█▁▅▂▂▃▃▃▄▃▄█▁▂▆▇▁▃▃▆▃▂▂▄▃▅▄▃▂▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.67904</td></tr><tr><td>eval/runtime</td><td>275.664</td></tr><tr><td>eval/samples_per_second</td><td>5.873</td></tr><tr><td>eval/steps_per_second</td><td>5.873</td></tr><tr><td>total_flos</td><td>9.076772549758157e+16</td></tr><tr><td>train/epoch</td><td>0.99993</td></tr><tr><td>train/global_step</td><td>7282</td></tr><tr><td>train/grad_norm</td><td>1.61203</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.5802</td></tr><tr><td>train_loss</td><td>0.7365</td></tr><tr><td>train_runtime</td><td>7443.2073</td></tr><tr><td>train_samples_per_second</td><td>1.957</td></tr><tr><td>train_steps_per_second</td><td>0.978</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">saiga_llama3_8b-ruFinGPT</strong> at: <a href='https://wandb.ai/sergey_1221_team/huggingface/runs/jxbi1wlt' target=\"_blank\">https://wandb.ai/sergey_1221_team/huggingface/runs/jxbi1wlt</a><br/> View project at: <a href='https://wandb.ai/sergey_1221_team/huggingface' target=\"_blank\">https://wandb.ai/sergey_1221_team/huggingface</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240815_005201-jxbi1wlt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75de1cfe-e8d2-4691-9da5-4232d5820a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sergey\\.conda\\envs\\edok\\Lib\\site-packages\\peft\\utils\\save_and_load.py:232: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#trainer.model.save_pretrained(new_model)\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775697eb-27f6-4b7e-8388-9ac0f6804d40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d237de86-8e67-4ac8-986f-fd913bd2105c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd01c9fd-5835-4658-ac40-f183baecb916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Both `max_new_tokens` (=1536) and `max_length`(=512) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "умеренно позитивно\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": \"Какая тональность у этой новости? Пожалуйста, выберите только один вариант ответа из {сильно негативно/умеренно негативно/слабо негативно/нейтрально/слабо позитивно/умеренно позитивно/сильно позитивно}.\"\n",
    "    },   \n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Чистая прибыль РусГидро по РСБУ за 1 полугодие выросла на 17%\"\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False, \n",
    "                                       add_generation_prompt=True)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors='pt', padding=True, \n",
    "                   truncation=True).to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=128, \n",
    "                         num_return_sequences=1)\n",
    "\n",
    "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(text.split(\"assistant\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf63280a-8d25-4797-8b05-a578b2ef4476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a2d48-6bf1-424e-86fe-ed81b332d9c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
